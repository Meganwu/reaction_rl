{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single atom training\n",
    "This notebook goes through the workflow of setting the hyperparameters, collecting atom manipulation data, and training the deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "from matplotlib import pyplot as plt, patches\n",
    "import torch\n",
    "from REACTRL import RealExpEnv, Episode_Memory, Createc_Controller, sac_agent, ReplayMemory, HerReplayMemory\n",
    "from REACTRL import plot_graph, show_reset, show_done, show_step\n",
    "matplotlib.rcParams['image.cmap'] = 'gray'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "from matplotlib import pyplot as plt, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the anchor image\n",
    "This cell retrieves the current STM scan image and use it as the template for positioning the anchor in STM images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeed to connect\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dd802cb610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "createc_controller = Createc_Controller(None, None, None, None)\n",
    "img_forward = np.array(createc_controller.stm.scandata(1,4))\n",
    "#TODO\n",
    "#Set the pixel of the top-left corner, widht, and height of the anchor\n",
    "#If the anchor is not used, just set w and h to a small number like below\n",
    "top_left, w, h = (0,0), 3, 3\n",
    "#template = img_forward[top_left[1]:top_left[1]+h, top_left[0]:top_left[0]+w]\n",
    "#plt.imshow(template)\n",
    "\n",
    "template = img_forward[top_left[1]:top_left[1]+h, top_left[0]:top_left[0]+w]\n",
    "plt.imshow(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the parameters and create a RealExpEnv object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succeed to connect\n",
      "Load cnn weight\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#TODO\n",
    "step_nm = 0.4 #Set the radius of the manipulation start position relative the the atom start position\n",
    "goal_nm  = 2 #Set the radius of the manipulation end position relative the the atom start position\n",
    "max_mvolt = 15 #Set the maximum bias voltage in mV \n",
    "max_pcurrent_to_mvolt_ratio = 6E3 #Set the maximum conductance in pA/mV\n",
    "max_len = 5 #Set maximum episode length\n",
    "template_max_y = 3 #Set the maximum or minimum row number to search for anchor\n",
    "#Set the path to load CNN weight for the atom movement classifier\n",
    "CNN_weight_path = 'C:/Users/wun2/github/reaction_rl/training_data_and_model_parameters/model parameters/_atom_move_detector_conv_2460.pth'\n",
    "current_jump  = 4 #Set the current jump gradient/ std(current) threshold required to take STM scan\n",
    "\n",
    "#Set STM scan parameters\n",
    "pixel = 128 \n",
    "im_size_nm = 7 #Image size in nm \n",
    "scan_mV = 500 #bias voltage\n",
    "x_nm, y_nm = createc_controller.get_offset_nm()\n",
    "offset_nm = np.array([x_nm, y_nm]) #Set offset to current offset value\n",
    "\n",
    "#Set manipulation parameters to pull atoms from image edge to center\n",
    "pull_back_mV = 5 #bias in mV\n",
    "pull_back_pA = 60000 #current in pA\n",
    "\n",
    "#Set manipulation limit [left, right, up, down] in nm\n",
    "manip_limit_nm = np.array([x_nm - 0.5*im_size_nm+0.25, x_nm + 0.5*im_size_nm-0.25, y_nm+0.25, y_nm+im_size_nm-0.25])\n",
    "\n",
    "env = RealExpEnv(step_nm, max_mvolt, max_pcurrent_to_mvolt_ratio, goal_nm, \n",
    "                 template, current_jump, im_size_nm, offset_nm, manip_limit_nm, pixel, \n",
    "                 template_max_y, scan_mV, max_len, \n",
    "                 CNN_weight_path, \n",
    "                 bottom=False, random_scan_rate = 0.8, pull_back_mV = pull_back_mV,\n",
    "                 pull_back_pA = pull_back_pA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sac_agent object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "batch_size= 64 #Set minibatch size\n",
    "LEARNING_RATE = 0.0003 #Set learning rate\n",
    "\n",
    "#Set the action space range\n",
    "ACTION_SPACE = namedtuple('ACTION_SPACE', ['high', 'low'])\n",
    "action_space = ACTION_SPACE(high = torch.tensor([1,1,1,1,1,1]), low = torch.tensor([-1,-1,-1,-1,1/3,1/2]))\n",
    "\n",
    "#Initialize the soft actor-critic agent\n",
    "alpha = 1.0\n",
    "agent = sac_agent(num_inputs = 4, num_actions = 6, action_space = action_space, device=device, hidden_size=256, lr=LEARNING_RATE,\n",
    "                 gamma=0.9, tau=0.005, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a HerReplayMemory object\n",
    "Here we use the hindsight experience replay with the 'future' strategy to sample goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "replay_size=1000000 #Set memory size\n",
    "\n",
    "memory = HerReplayMemory(replay_size, env, strategy = 'future')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.n_sampled_goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Episode_Memory object\n",
    "The episode memory class is used to store all the relavant information in each training episode, including the STM images, state, action, reward, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_memory = Episode_Memory()\n",
    "#TODO\n",
    "#Set the folder name to store training data and neural network weight\n",
    "folder_name =  'C:/Users/wun2/github/reaction_rl/test_nian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the hyperparameters for Emphasize Recent Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_k_min = 500\n",
    "eta = 0.994\n",
    "max_ep_len = max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create empty lists for logging performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards, alphas, precisions, episode_lengths = [], [], [], []\n",
    "avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sac_train(max_steps = max_len, num_episodes = 50, episode_start = 0):\n",
    "    \"\"\"\n",
    "    Collect training data and train the RL agent\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_steps: int\n",
    "            maximum steps in an episode\n",
    "            \n",
    "    num_episodes: int\n",
    "            Train for this many episodes\n",
    "    \n",
    "    episode_start: int\n",
    "            Index to use for the starting episode\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None : None\n",
    "    \"\"\"\n",
    "    for i_episode in range(episode_start,episode_start+num_episodes):\n",
    "        print('Episode:', i_episode)\n",
    "        episode_reward, episode_steps = 0, 0\n",
    "        done = False\n",
    "        state, info = env.reset(update_conv_net=False)\n",
    "        show_reset(env.img_info, env.atom_start_absolute_nm, env.destination_absolute_nm,\n",
    "                   env.template_nm, env.template_wh)\n",
    "        episode_memory.update_memory_reset(env.img_info, i_episode, info)\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = agent.select_action(state)\n",
    "            old_atom_nm = env.atom_absolute_nm\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            episode_steps+=1\n",
    "            episode_reward+=reward\n",
    "            mask = float(not done)\n",
    "            memory.push(state,action,reward,next_state,mask)\n",
    "            episode_memory.update_memory_step(state, action, next_state, reward, done, info)\n",
    "            show_step(env.img_info, info['start_nm']+old_atom_nm, info['end_nm']+old_atom_nm,\n",
    "                        env.atom_absolute_nm, env.atom_start_absolute_nm, \n",
    "                        env.destination_absolute_nm, action[4]*env.max_mvolt, \n",
    "                        action[5]*env.max_pcurrent_to_mvolt_ratio*action[4]*env.max_mvolt, \n",
    "                        env.template_nm, env.template_wh)\n",
    "            print('step:', step,'reward', reward, 'precision:', env.dist_destination)\n",
    "            if done:\n",
    "                episode_memory.update_memory_done(env.img_info, env.atom_absolute_nm, env.atom_relative_nm)\n",
    "                episode_memory.save_memory(folder_name)\n",
    "                print('Episode reward:', episode_reward)\n",
    "                break\n",
    "            else:                \n",
    "                state=next_state\n",
    "             \n",
    "        if (len(memory)>batch_size):\n",
    "            episode_K = int(episode_steps)\n",
    "            for k in range(episode_K):\n",
    "                c_k = max(int(memory.__len__()*eta**((k)*(1000/episode_K))), 500)\n",
    "                agent.update_parameters(memory, batch_size, c_k)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        alphas.append(agent.alpha.item())\n",
    "        precisions.append(env.dist_destination)\n",
    "        episode_lengths.append(episode_steps)\n",
    "        avg_episode_rewards.append(np.mean(episode_rewards[-min(100,len(episode_rewards)):]))\n",
    "        avg_alphas.append(np.mean(alphas[-min(100, len(alphas)):]))\n",
    "        avg_precisions.append(np.mean(precisions[-min(100, len(precisions)):]))\n",
    "        avg_episode_lengths.append(np.mean(episode_lengths[-min(100, len(episode_lengths)):]))\n",
    "        \n",
    "        if (i_episode+1)%2==0:\n",
    "            plot_graph(episode_rewards, precisions, alphas, episode_lengths,\n",
    "                      avg_episode_rewards, avg_alphas, avg_precisions, avg_episode_lengths)\n",
    "            \n",
    "        if (i_episode)%20 == 0:\n",
    "            torch.save(agent.critic.state_dict(), '{}/_critic_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.policy.state_dict(), '{}/_policy_{}.pth'.format(folder_name,i_episode))\n",
    "            torch.save(agent.alpha, '{}/_alpha_{}.pth'.format(folder_name,i_episode))\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2887\n",
      "The scan will take 12.5 seconds\n",
      "offset_test0000 [39.9988076 39.9988076]\n",
      "offset_test1111 [39.9988076 39.9988076]\n",
      "offset_test2222 [39.9988076 39.9988076]\n",
      "[[ 4.57409828e-03  4.75422011e-03  4.64299444e-03 ...  1.25476703e-03\n",
      "   1.31258013e-03  1.13125940e-03]\n",
      " [-9.43183073e-05 -3.55986101e-04 -4.38601542e-04 ...  1.41551877e-03\n",
      "   1.34601635e-03  1.58836543e-03]\n",
      " [ 3.86152743e-04  5.16683512e-04  4.55048906e-04 ... -1.50886591e-03\n",
      "  -1.49015346e-03 -1.54916546e-03]\n",
      " ...\n",
      " [ 1.45730134e-03  1.56971230e-03  1.56267555e-03 ...  7.78694278e-04\n",
      "   7.22304880e-04  6.37066835e-04]\n",
      " [-4.69063163e-04 -5.56685394e-04 -5.22237313e-04 ... -8.64428956e-04\n",
      "  -7.07672144e-04 -5.66889376e-04]\n",
      " [-1.58194348e-03 -1.40682843e-03 -1.39598379e-03 ... -5.00783009e-04\n",
      "  -6.63983930e-04 -8.53172476e-04]] 0.003634965699172096\n",
      "[[ 1.62357570e-03  1.64786345e-03  1.30594026e-03 ...  2.58172629e-03\n",
      "   2.51779917e-03  2.59930737e-03]\n",
      " [ 1.74509150e-03  1.64468633e-03  1.37786499e-03 ... -1.38515970e-03\n",
      "  -1.39210479e-03 -1.46127712e-03]\n",
      " [-2.27138556e-03 -2.15864452e-03 -1.97318581e-03 ... -1.53424899e-03\n",
      "  -1.42651474e-03 -1.23318822e-03]\n",
      " ...\n",
      " [-4.34626457e-04 -2.33432123e-04  1.18696477e-05 ...  3.14589009e-03\n",
      "   3.49776496e-03  3.74044413e-03]\n",
      " [ 3.06838005e-03  3.21998332e-03  2.93504217e-03 ... -1.84179885e-04\n",
      "  -3.05327470e-04 -7.57613250e-05]\n",
      " [-9.29977201e-04 -1.06662199e-03 -1.00847881e-03 ... -1.06592947e-03\n",
      "  -1.44194651e-03 -1.78673073e-03]] 0.0055801738108093735\n",
      "Warning: atom is out of limit\n",
      "pulling atom back to center\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wun2\\github\\reaction_rl\\REACTRL\\env_modules\\get_atom_coordinate.py:382: FutureWarning: `selem` is a deprecated argument name for `binary_dilation`. It will be removed in version 1.0. Please use `footprint` instead.\n",
      "  r = morphology.binary_dilation(maxima, selem=diamond)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scan will take 12.5 seconds\n",
      "offset_test0000 [39.9988076 39.9988076]\n",
      "offset_test1111 [39.9988076 39.9988076]\n"
     ]
    }
   ],
   "source": [
    "sac_train(episode_start = 2887,num_episodes = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a section to condition tip\n",
    "start_crash_hole=0\n",
    "approach_z=2\n",
    "\n",
    "def tip_cond_area(center_x=0, center_y=0, approach_area_length=800, cond_area_length=200, margin=20):\n",
    "    cond_area_center_x, cond_area_center_y=center_x-approach_area_length+cond_area_length+margin, center_y-approach_area_length+cond_area_center_y+margin\n",
    "    return cond_area_center_x, cond_area_center_y, cond_area_length\n",
    "\n",
    "env.default_max_radius_cellsize(cellsize=20, max_radius=200)\n",
    "points=env.computeLocationIDs()\n",
    "cond_area_center_x, cond_area_center_y, cond_area_length=tip_cond_area()\n",
    "\n",
    "env.createc_controller.ramp_bias_mV(-1.2)\n",
    "env.createc_controller.tip_form(approach_z, cond_area_center_x+points[start_crash_hole][0]*cellsize,cond_area_center_y+points[start_crash_hole][1]*cellsize)\n",
    "print(env.createc_controller.im_size_nm)\n",
    "start_crash_hole=start_crash_hole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x = 0\n",
    "start_y = 0\n",
    "tip_x = 0\n",
    "tip_y = 0\n",
    "check_similarity=None\n",
    "check_similarity.append([tip_x, tip_y])\n",
    "radius = 35\n",
    "upper_limit_move=400\n",
    "env.max_radius=150\n",
    "\n",
    "image_forward = info['img_info']['img_forward']\n",
    "image_backward =info['img_info']['img_backward']\n",
    "\n",
    "\n",
    "# dectect atoms\n",
    "isAtom = False\n",
    "\n",
    "\n",
    "\n",
    "# debris\n",
    "debris_thres = 6\n",
    "DebresCounter = 0\n",
    "noDebris = False\n",
    "\n",
    "\n",
    "\n",
    "# Crash dectected\n",
    "crash_thres = 1e-25 \n",
    "approach_z = 4\n",
    "noCrash = False\n",
    "\n",
    "\n",
    "while not isAtom or not noDebris or not noCrash:\n",
    "    \n",
    "    # detect atoms\n",
    "    atom_forward = blob_detection(info[img_forward])[0]\n",
    "\n",
    "    if len(atom_forward)>0:\n",
    "        isAtom = True\n",
    "\n",
    "    # detect debris\n",
    "    noDebris =  ((np.max(image_forward) - np.min(image_forward)) < debris_thres) \n",
    "    \n",
    "    # dectect crash\n",
    "    noCrash = (np.max(img_forward)-np.min(img_forward) )< crash_thres and (np.mean(img_forward)>0)\n",
    "    \n",
    "    if isAtom and noDebris and noCrash:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        tip_x, tip_y=env.GetNextGoodClosest(tip_x, tip_y, initial_x=start_x, initial_y=start_y, forbiden_radius=radius, upper_limit_move=upper_limit_move,approach_limit=[-180, 180, -180, 180],check_similarity=check_similarity) \n",
    "        if tip_x==None:\n",
    "            tip_x, tip_y=check_similarity[-1][0], check_similarity[-1][1]\n",
    "            image=env.set_newtip_pos(tip_x, tip_y)\n",
    "            image_forward, image_backward =image[0], image[1]\n",
    "            radius=radius*2\n",
    "            env.max_radius=env.max_radius*2\n",
    "            upper_limit_move=upper_limit_move+20000\n",
    "        else:\n",
    "            radius=35\n",
    "            upper_limit_move=400\n",
    "            env.max_radius=150\n",
    "                \n",
    "    \n",
    "            \n",
    "            \n",
    "    check_similarity.append([tip_x, tip_y])        \n",
    "    if radius>280:\n",
    "        print('there is no section satisfying the demand within radius of 280 nm')\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condition probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.createc_controller.stm.setparam('TipForm_Volt', -3) # Set the bias voltage for the atom manipulation(unit V)\n",
    "\n",
    "env.createc_controller.stm.setparam('TipForm_Z', 0.0) # Set the z position for the atom manipulation(unit angstrom)\n",
    "\n",
    "env.createc_controller.stm.setparam('Tip_Delay', 0.4915) # Set the delay time for the atom manipulation(unit s)\n",
    "env.createc_controller.stm.setparam('Tip_Latddx', 833) # Set the lateral ddx for the atom manipulation(unit angstrom/ms^2)\n",
    "env.createc_controller.stm.setparam('Tip_LatDelay', 20) # Set the lateral delay for the atom manipulation(unit ms)\n",
    "env.createc_controller.stm.setparam('Tip_Gain', 6) # Set the gain for the atom manipulation(unit 1)\n",
    "env.createc_controller.stm.setparam('TipForm_Zoffset', 0.0) # Set the z offset for the atom manipulation(unit angstrom)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_condition_actions={'stall': [0, 0],\n",
    "                      'soft pulse pos':[0, 2.5],\n",
    "                      'medium pulse pos': [0, 4.0],\n",
    "                      'soft pulse neg': [0 , -2.5], \n",
    "                      'medium pulse neg': [0, -4],\n",
    "                      'strong pulse neg': [0, -8],\n",
    "                      'very soft dip': [0, -8],\n",
    "                      'soft dip 1': [-0.80, 0.02],\n",
    "                      'soft dip 2': [-1.2, 0.02],\n",
    "                      'soft dip 3': [-1.8, 0.02],\n",
    "                      'medium soft': [-2.5, 0.02],\n",
    "                      'strong dip': [-5.0, 0.02]\n",
    "                      }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
